{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqrpGatv2tkhWxeGrycNCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorsenoguchi/MAC0425/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IB4v9oG2ESJ"
      },
      "source": [
        "# Initial Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONpz8pPT2QN_"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MlzkhVf2XEe"
      },
      "source": [
        "# Q-Learning Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9A2-hpq2vIP"
      },
      "source": [
        "class Q_learning():\n",
        " \n",
        "  def __init__(self, S, A, Q, R, N, initial, terminal): # Starts an object of the class.\n",
        "    self.S = S                # List with states.\n",
        "    self.A = A                # Dictionary with a list of actions for each state.\n",
        "    self.R = R                # Dictionary with the reward for each state.\n",
        "    self.Q = Q                # Dictionary with the quality for each state and action.\n",
        "    self.N = N                # Dictionary with the number of visits for each state and action.\n",
        "    self.initial = initial    # List of possibles initial states.\n",
        "    self.terminal = terminal  # List of possibles terminal states.\n",
        " \n",
        "  def go(self, s, a): # Return the next state given the current state and an action.\n",
        "    saux = np.asarray(s)\n",
        "    if a == \"west\":\n",
        "      saux = saux + np.asarray([[-1,0],[0,1],[0,-1]])\n",
        "    elif a == \"north\": \n",
        "      saux = saux + np.asarray([[0,1],[1,0],[-1,0]])\n",
        "    elif a == \"east\":\n",
        "      saux = saux + np.asarray([[1,0],[0,1],[0,-1]])\n",
        "    elif a == \"south\":\n",
        "      saux = saux + np.asarray([[0,-1],[1,0],[-1,0]]) \n",
        "    idx = np.random.choice(np.arange(3), p = [0.8,0.1,0.1])\n",
        "    s_prime = tuple(saux[idx])\n",
        "    return s_prime if s_prime in self.S else s\n",
        " \n",
        "  def f(self, s, a): # Aversion to boredom function.\n",
        "    return 1 if self.N[s,a] == 0 else self.Q[s,a] / self.N[s,a]\n",
        " \n",
        "  def alpha(self, s, a):  \n",
        "    return 1 if self.N[s,a] == 0 else 1 / self.N[s, a]\n",
        " \n",
        "  def show_policy(self): # Print the current policy.\n",
        "    policy = dict()\n",
        "    for s in S:\n",
        "      idx = np.argmax([self.f(s, a) for a in self.A[s]])\n",
        "      policy[s] = self.A[s][idx]  \n",
        "    for j in range(3):\n",
        "      print(end = \"|\")\n",
        "      for i in range(4):\n",
        "        if policy[(i,2-j)] == \"west\": print(\"\\u2190\", end = \"|\")\n",
        "        elif policy[(i,2-j)] == \"north\": print(\"\\u2191\", end = \"|\")\n",
        "        elif policy[(i,2-j)] == \"east\": print(\"\\u2192\", end = \"|\")\n",
        "        elif policy[(i,2-j)] == \"south\": print(\"\\u2193\", end = \"|\")\n",
        "      print()\n",
        "    print(\"\\n\")\n",
        " \n",
        "  def fit(self, random_state = None, epsilon = 1e-3, max_epochs = 1e6, gamma = 1): # Train the model.\n",
        "    if random_state is not None:\n",
        "      np.random.seed(random_state)\n",
        "    epoch = 0\n",
        "    converged = False\n",
        "    while epoch < max_epochs and not converged:\n",
        "      delta = 0\n",
        "      # Initial State.\n",
        "      s = random.choice(self.initial)\n",
        "      # Reward of the initial state.\n",
        "      r = self.R[s]\n",
        "      # Index of the initial state.\n",
        "      idx = np.argmax([self.f(s, a_prime) for a_prime in self.A[s]])       \n",
        "      # First action.\n",
        "      a = self.A[s][idx]\n",
        "      while s not in self.terminal:\n",
        "        # Next State.\n",
        "        s_prime = self.go(s, a)\n",
        "        # Next Reward.\n",
        "        r_prime = self.R[s_prime]\n",
        "        self.N[s, a] += 1\n",
        "        # Old value of quality in state s and action a.\n",
        "        Q_old = self.Q[s, a]  \n",
        "        self.Q[s, a] += self.alpha(s, a) * (r + gamma * max([self.Q[s_prime, a_prime] for a_prime in self.A[s_prime]]) - self.Q[s, a])\n",
        "        # New value of quality in state s and action a.\n",
        "        Q_new = self.Q[s, a]\n",
        "        # Calculates the greatest variation between an old quality and a new one at the time. \n",
        "        delta = max(delta, np.abs(Q_new - Q_old))\n",
        "        # Updates current status and current reward.\n",
        "        s, r = s_prime, r_prime\n",
        "        idx = np.argmax([self.f(s, a_prime) for a_prime in self.A[s]])       \n",
        "        # Next Action.\n",
        "        a = self.A[s][idx]\n",
        "        #self.show_policy()\n",
        "      # Updates the quality of the terminal state.\n",
        "      for a in [\"north\", \"east\", \"south\", \"west\"]:\n",
        "        self.Q[s, a] = self.R[s]\n",
        "      # Checks whether the algorithm has converged.\n",
        "      if delta <= epsilon: converged = True\n",
        "      #print(\"\\u0394Q = {}\\n\".format(delta))\n",
        "      epoch += 1\n",
        " \n",
        "    if converged:    \n",
        "      print(\"The Q-Learning algorithm took {} epochs to converge.\\n\".format(epoch))\n",
        "    else:\n",
        "      print(\"The Q-Learning algorithm took {} epochs and did not converged.\\n\".format(max_epochs))\n",
        "    print(\"The final policy was\")\n",
        "    self.show_policy()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhNZpbLpUGck"
      },
      "source": [
        "def generate_states(x, y): \n",
        "  x = [i for i in range(x)]\n",
        "  y = [i for i in range(y)]\n",
        "  S = list(itertools.product(x, y))\n",
        "  return S\n",
        " \n",
        "def generate_actions(S):\n",
        "  A = dict()\n",
        "  for s in S:\n",
        "    actions = []\n",
        "    if (s[0], s[1]+1) in S:\n",
        "      actions.append(\"north\")\n",
        "    if (s[0]+1, s[1]) in S:\n",
        "      actions.append(\"east\")\n",
        "    if (s[0], s[1]-1) in S:\n",
        "      actions.append(\"south\")\n",
        "    if (s[0]-1, s[1]) in S:\n",
        "      actions.append(\"west\")\n",
        "    A[s] = actions\n",
        "  return A"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrMHQkv6sa_M",
        "outputId": "4d3663d2-f9ee-4a9d-c3fc-1c1884184157"
      },
      "source": [
        "# Reproducibility Seed.\n",
        "seed = 31\n",
        " \n",
        "# x axis.\n",
        "x = 4\n",
        "# y axis.\n",
        "y = 3\n",
        "\n",
        "# States.\n",
        "S = generate_states(x, y)\n",
        "# Actions.\n",
        "A = generate_actions(S)\n",
        " \n",
        "# Dictionary with rewards.\n",
        "R = {s: -1 for s in S}\n",
        "R[(1,1)], R[(3,2)] = -100, 100\n",
        " \n",
        "# Dictionaries with the quality and number of visits.\n",
        "Q, N = dict(), dict()\n",
        "for s in S:\n",
        "  Q.update({(s, a): 0 for a in A[s]})\n",
        "  N.update({(s, a): 0 for a in A[s]})\n",
        " \n",
        "# List with possibles initial states.\n",
        "initial_states = [(0,0)]\n",
        "# List with possibles terminal states.\n",
        "terminal_states = [(3,2)]\n",
        " \n",
        "# Discount Factor.\n",
        "gamma = 1\n",
        " \n",
        "model = Q_learning(S, A, Q, R, N, initial_states, terminal_states)\n",
        "model.fit(random_state = seed)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Q-Learning algorithm took 13041 epochs to converge.\n",
            "\n",
            "The final policy was\n",
            "|→|→|→|↓|\n",
            "|↑|↑|↑|↑|\n",
            "|↑|→|↑|↑|\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}